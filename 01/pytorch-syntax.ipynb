{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在开始介绍 pytorch 的一个使用语法结构前,我们先说一下 深度学习领域几位真正的大神,可以说深度学习领域的目前的产出都离不开这几位大神\n",
    "Hinton, LeCun, Bengio\n",
    "\n",
    "Hinton 杰弗里·埃弗里斯特·辛顿\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Geoffrey_Hinton_at_UBC.jpg/440px-Geoffrey_Hinton_at_UBC.jpg\" style=\"zoom:50%;margin-left:0px\" />\n",
    "\n",
    "杰弗里·埃弗里斯特·辛顿 FRS（英语：Geoffrey Everest Hinton）（1947年12月6日－）是一位英国出生的加拿大计算机学家和心理学家，以其在神经网络方面的贡献闻名。辛顿是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者\n",
    "\n",
    "后面两位就不细说了,总之都是很牛,可以说这三位以及他们的学生 点亮了深度学习领域的一盏明灯\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch 基础及重要 api 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先 推荐优先使用虚拟环境,接下来我们准备安装依赖包 \n",
    "\n",
    "```\n",
    "pip3 install http://download.pytorch.org/whl/torch-0.3.1-cp36-cp36m-macosx_10_7_x86_64.whl \n",
    "pip3 install torchvision \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor 张量\n",
    "Pytorch 里面的基本对象就是张量 Tensor, 其表示的是已给多维的矩阵,其和 numpy 的数组是对应的 Tensor ,二者的区别就是 Pytorch 的 Tensor 可以运行在 GPU 上, TensorFlow 的 tensor 也是\n",
    "\n",
    "依据数据类型 tensor 分一下几种\n",
    "- torch.FloatTensor 32\n",
    "- torch.DoubleTensor 64\n",
    "- torch.shortTensor 16\n",
    "- torch.IntTensor 32\n",
    "\n",
    "下面我们给一些实例说明演示一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2  3\n",
      " 4  8\n",
      " 7  9\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "a size is torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor([[2,3], [4,8], [7,9]])\n",
    "print(\"{}\".format(a))\n",
    "print(\"a size is {}\".format(a.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero tensor:\n",
      " 0  0\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "randn init the tensor:\n",
      "-0.2921  0.8625\n",
      " 0.2409 -0.4369\n",
      " 0.2295  1.0684\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#使用 torch 创建全是 0 的张量\n",
    "c = torch.zeros((3,2))\n",
    "print(\"zero tensor:{}\".format(c))\n",
    "\n",
    "#随机初始化指定矩阵\n",
    "d = torch.randn((3,2))\n",
    "print(\"randn init the tensor:{}\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conver to numpy is \n",
      " [[-0.29212478  0.86247635]\n",
      " [ 0.24088798 -0.4369324 ]\n",
      " [ 0.22953479  1.0684093 ]]\n",
      "the torch_e is : \n",
      " 2  3\n",
      " 4  5\n",
      "[torch.LongTensor of size 2x2]\n",
      "\n",
      "the torch_e change data type is : \n",
      " 2  3\n",
      " 4  5\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#与 numpy 互转\n",
    "numpy_b = d.numpy()\n",
    "print(\"conver to numpy is \\n {}\".format(numpy_b))\n",
    "#np array conver to tensor\n",
    "e = np.array([[2, 3], [4, 5]])\n",
    "torch_e = torch.from_numpy(e)\n",
    "print(\"the torch_e is : {}\".format(torch_e))\n",
    "print(\"the torch_e change data type is : {}\".format(torch_e.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable \n",
    "变量的本质其实和张量相同,只不过 torch 当中的变量可以自动求导,不过变量不会被放入计算图中, torch 的变量实在 torch.autograd.Variable 中 将一个 tensor 变成 Variable 很简单 只需要 Variable(a) 就可以了\n",
    "> Variable 包含三个比较重要的组成属性: ***data*** , ***grad*** , ***grad_fn*** 通过 data 可以取出变量里面的张量数值, grad_fn 是计算梯度的操作,比如是通过加减还是乘除得来的,最后 grad 是反向传播的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.Tensor([1]), requires_grad = True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad = True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad = True)\n",
    "# Build a computational graph\n",
    "y = w * x + b   # y = 2 * x +3\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)    # x.grad = 2\n",
    "print(w.grad)    # w.grad = 1\n",
    "print(b.grad)    # b.grad = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:Variable containing:\n",
      " 0.6424\n",
      " 1.6938\n",
      "-0.5470\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 2.0000\n",
      " 0.2000\n",
      " 0.0200\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 除了上面针对标量求导以外,我们还可以针对矩阵求导\n",
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "y = x * 2\n",
    "print(\"y:{}\".format(y))\n",
    "y.backward(torch.FloatTensor([1, 0.1, 0.01]))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集\n",
    "在处理任何机器学习问题之前都需要数据读取,并进行预处理. Pytorch 提供了很多工具使得数据的读取和预处理变得很容易.\n",
    "\n",
    "torch.utils.data.DataSet 是代表这一数据的抽象类,你可以自己定义你的数据类继承 和 重写 这个抽象类, 你可以自己定义你的数据类继承 和重写这个抽象类,非常简单, 只需要定义 ***_ _len_ _*** ,***_ _getitem_ _*** 这两个列表,例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, csv_file, txt_file, root_dir, other_file):\n",
    "        self.csv_file = csv_file\n",
    "        with open(txt_file,'r') as f:\n",
    "            data_list = f.readlines()\n",
    "        self.txt_data = data_list\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = txt_data[idx]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量迭代可以使用 \n",
    "#dataiter = DataLoader(MyDataSet, batch_size=32, shuffle=True, collate_fn=default_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模组 nn.Module\n",
    "在 pyTorch 里面编写神经网络,所有的层结构 和损失函数都来自于 torch.nn, 所有的模型都从这个基类继承而来  nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net_name(torch.nn.Module):\n",
    "    def __init__(self, other_arguments):\n",
    "        super(\"net_name\", self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=None, out_channels=None, kernel_size=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}